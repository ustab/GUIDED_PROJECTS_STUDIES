{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLEANING.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMi5tvkShYbYhDBEl9TmYKs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ustab/GUIDED_PROJECTS_STUDIES/blob/main/CLEANING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYEo7m3QVqVK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######This guide will explain the basics of what data cleaning is, then jump into the real stuff. Downstream, this guide will transform into a how-to for data cleaning with Python walking you through step by step. \n",
        "\n",
        "1. What is Data Cleaning?\n",
        "Data cleaning is the process of correcting or removing corrupt, incorrect, or unnecessary data from a data set before data analysis.\n",
        "\n",
        "What more does one need? Regardless, let’s get into the nitty gritty of cleaning our data with these libraries.\n",
        "\n",
        "2. Data Cleaning With Python\n",
        "Using Pandas and NumPy, we are now going to walk you through the following series of tasks, listed below. \n",
        "\n",
        "Here are the basic data cleaning tasks we’ll tackle:\n",
        "\n",
        "Importing Libraries\n",
        "Input Customer Feedback Dataset\n",
        "Locate Missing Data\n",
        "Check for Duplicates\n",
        "Detect Outliers \n",
        "Normalize Casing \n",
        "\n",
        "## Importing Libraries\n",
        "Let’s get Pandas and NumPy up and running on your Python script.\n",
        "\n",
        "INPUT:\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "##Input Customer Feedback Dataset\n",
        "Next, we ask our libraries to read a feedback dataset. Let’s see what that looks like.\n",
        "\n",
        "INPUT:\n",
        "\n",
        "data = pd.read_csv('feedback.csv')\n",
        "OUTPUT: \n",
        "As you can see the “feedback.csv” should be the dataset you want to examine. And, in this case, when we read “pd.read_csv” as the prior function, we know we are using the Pandas library to read our dataset. \n",
        "\n",
        "##Locate Missing Data\n",
        "Next, we are going to use a secret Python hack known as ‘isnull function’ to discover our data. Actually a common function, 'isnull' helps us find where in our dataset there are missing values. This is useful information as this is what we need to correct while data cleaning.\n",
        "\n",
        "INPUT:\n",
        "\n",
        "data.isnull()\n",
        "\n",
        "So, for example, datapoint 1 has missing data in its Review section and its Review ID section (both are marked true). \n",
        "\n",
        "We can further expand the missing data of each feature by coding:\n",
        "\n",
        "INPUT: \n",
        "\n",
        "data.isnull().sum()\n",
        "OUTPUT:\n",
        "\n",
        "\n",
        "From here, we use code to actually clean the data. This boils down to two basic options. 1) Drop the data or, 2) Input missing data. If you opt to:\n",
        "\n",
        " #Drop the data\n",
        "You’ll have to make another decision – whether to drop only the missing values and keep the data in the set, or to eliminate the feature (the entire column) wholesale because there are so many missing datapoints that it isn’t fit for analysis.\n",
        "\n",
        "If you want to drop the missing values you’ll have to go in and mark them void according to Pandas or NumBy standards (see section below). But if you want to drop the entire column, here’s the code:\n",
        "\n",
        "remove = ['Review ID','Date']\n",
        "data.drop(remove, inplace =True, axis =1)\n",
        "OUTPUT: \n",
        "Now, let’s examine our other option.\n",
        "\n",
        "#Input missing data\n",
        "\n",
        "Technically, the method described above of filling in individual values with Pandas or NumBy standards is also a form of inputting missing data – we call it adding ‘No Review’. When it comes to inputting missing data you can either add ‘No Review’ using the code below, or manually fill in the correct data.\n",
        "\n",
        "data['Review'] = data['Review'].fillna('No review')\n",
        "\n",
        "As you can see, now the data point 1 have now been marked as ‘No Review’ – success!\n",
        "\n",
        "#Check for Duplicates\n",
        "\n",
        "Duplicates, like missing data, cause problems and clog up analytics software. Let’s locate and eliminate them.\n",
        "\n",
        "duplicates we start out with:\n",
        "\n",
        "data.duplicated()\n",
        "\n",
        "data.drop_duplicates()\n",
        " \n",
        "And there we have it, our dataset with our duplicate removed. Onwards.\n",
        "\n",
        "#Detect Outliers\n",
        "\n",
        "Outliers are numerical values that lie significantly outside of the statistical norm. Cutting that down from unnecessary science garble – they are data points that are so out of range they are likely misreads. \n",
        "\n",
        "data['Rating'].describe()\n",
        "\n",
        "data.loc[10,'Rating'] = 1\n",
        "\n",
        "\n",
        "#Normalize Casing\n",
        "\n",
        "Last but not least we are going to dot our i’s and cross our t’s. \n",
        "Meaning we are going to standardize (lowercase) all review titles so as not to confuse our algorithms, and we are going to capitalize Customer Names, so that our algorithms know they are variables (you’ll see this in action below).\n",
        "\n",
        "Here’s how to make every review title lowercase:\n",
        "    data['Review Title'] = data['Review Title'].str.lower()\n",
        "\n",
        "Here’s how to ensure Customer Name capitalization:\n",
        "\n",
        "data['Customer Name'] = data['Customer Name'].str.title()\n",
        "\n",
        "##Takeaways\n",
        "\n",
        "Staying ahead of competition when it comes to data analysis isn’t easy - it seems like there are more powerful software and new functionalities being developed and launched every day"
      ],
      "metadata": {
        "id": "H0tE-o1WVrFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WMjmep39Yvq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OpJBrHFaYwUG"
      }
    }
  ]
}