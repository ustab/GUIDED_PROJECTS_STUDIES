{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled51.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOLG5rgtl1EHybrTk+GXos4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ustab/GUIDED_PROJECTS_STUDIES/blob/main/Untitled51.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are 8 effective data cleaning techniques:\n",
        "\n",
        "Remove duplicates\n",
        "Remove irrelevant data\n",
        "Standardize capitalization\n",
        "Convert data type\n",
        "Clear formatting\n",
        "Fix errors\n",
        "Language translation\n",
        "Handle missing values\n",
        "Let’s go through these in more detail now.\n",
        "\n",
        "1. Remove Duplicates\n",
        "When you collect your data from a range of different places, or scrape your data, it’s likely that you will have duplicated entries. These duplicates could originate from human error where the person inputting the data or filling out a form made a mistake.\n",
        "\n",
        "Duplicates will inevitably skew your data and/or confuse your results. They can also just make the data hard to read when you want to visualize it, so it’s best to remove them right away.\n",
        "# Drop all duplicates in the DataFrame\n",
        "df = df.drop_duplicates() \n",
        "\n",
        "# Python code to remove duplicate elements\n",
        "def Remove(duplicate):\n",
        "    final_list = []\n",
        "    for num in duplicate:\n",
        "        if num not in final_list:\n",
        "            final_list.append(num)\n",
        "    return final_list\n",
        "    # Python 3 code to demonstrate\n",
        "\n",
        "# Removing duplicate elements from the list\n",
        "\n",
        "l = [1, 2, 4, 2, 1, 4, 5]\n",
        "print(\"Original List: \", l)\n",
        "\n",
        "res = [*set(l)]\n",
        "print(\"List after removing duplicate elements: \", res)\n",
        "Driver Code;\n",
        "duplicate = [2, 4, 10, 20, 5, 2, 20, 4]\n",
        "print(Remove(duplicate))\n",
        "\n",
        "# SQL_Delete duplicate rows from a table example\n",
        "\n",
        "The following statement uses a common table expression (CTE) to delete duplicate rows:\n",
        "\n",
        "WITH cte AS (\n",
        "    SELECT \n",
        "        contact_id, \n",
        "        first_name, \n",
        "        last_name, \n",
        "        email, \n",
        "        ROW_NUMBER() OVER (\n",
        "            PARTITION BY \n",
        "                first_name, \n",
        "                last_name, \n",
        "                email\n",
        "            ORDER BY \n",
        "                first_name, \n",
        "                last_name, \n",
        "                email\n",
        "        ) row_num\n",
        "     FROM \n",
        "        sales.contacts\n",
        ")\n",
        "DELETE FROM cte\n",
        "WHERE row_num > 1;\n",
        "#\n",
        "\n",
        "2. Remove Irrelevant Data\n",
        "Irrelevant data will slow down and confuse any analysis that you want to do. So, deciphering what is relevant and what is not is necessary before you begin your data cleaning. For instance, if you are analyzing the age range of your customers, you don’t need to include their email addresses.\n",
        "\n",
        "Other elements you’ll need to remove as they add nothing to your data include:\n",
        "\n",
        "Personal identifiable (PII) data\n",
        "URLs\n",
        "HTML tags\n",
        "Boilerplate text (for ex. in emails)\n",
        "Tracking codes\n",
        "Excessive blank space between text\n",
        "3. Standardize Capitalization\n",
        "Within your data, you need to make sure that the text is consistent. If you have a mixture of capitalization, this could lead to different erroneous categories being created. \n",
        "\n",
        "It could also cause problems when you need to translate before processing as capitalization can change the meaning. For instance, Bill is a person's name whereas a bill or to bill is something else entirely. \n",
        "\n",
        "If, in addition to data cleaning, you are text cleaning in order to process your data with a computer model, it’s much simpler to put everything in lowercase. \n",
        "\n",
        "#HATALI TEXTLERI PYTHON KODU ILE DUZELTME\n",
        "\n",
        "text = \"Hey Amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first FIX THIS ASAP! @AmazonHelp\"\n",
        "\n",
        "text = text.lower()\n",
        "\n",
        "print(text)\n",
        "\n",
        "# REGEX (UNICODE TEXT IFADELERININ KALDIRILMASI)\n",
        "import re\n",
        "\n",
        "text = \"hey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first please fix asap! @amazonhelp\"\n",
        "\n",
        "text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
        "\n",
        "print(text)\n",
        "\n",
        "# REMOVING STOPWORDS\n",
        "\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "text = \"my package from amazon never arrived fix this asap\"\n",
        "text = \" \".join(\\[word for word in text.split() if word not in (stop)])\n",
        "\n",
        "print(text)\n",
        "\n",
        "#STEMMING\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "words = \\[\"jump\", \"jumped\", \"jumps\", \"jumping\"]\n",
        "stemmer = PorterStemmer()\n",
        "for word in words:\n",
        "\n",
        "print(word + \" = \" + stemmer.stem(word))\n",
        "\n",
        "4. Convert Data Types\n",
        "Numbers are the most common data type that you will need to convert when cleaning your data. Often numbers are imputed as text, however, in order to be processed, they need to appear as numerals. \n",
        "\n",
        "If they are appearing as text, they are classed as a string and your analysis algorithms cannot perform mathematical equations on them.\n",
        "\n",
        "The same is true for dates that are stored as text. These should all be changed to numerals. For example, if you have an entry that reads September 24th 2021, you’ll need to change that to read 09/24/2021.\n",
        "\n",
        "5. Clear Formatting\n",
        "Machine learning models can’t process your information if it is heavily formatted. If you are taking data from a range of sources, it’s likely that there are a number of different document formats. This can make your data confusing and incorrect. \n",
        "\n",
        "You should remove any kind of formatting that has been applied to your documents, so you can start from zero. This is normally not a difficult process, both excel and google sheets, for example, have a simple standardization function to do this. \n",
        "##str.center(<width>[, <fill>])\n",
        "str.expandtabs(tabsize=8)\n",
        "str.ljust(<width>[, <fill>])\n",
        "str.rjust(<width>[, <fill>])\n",
        "str.lstrip([<chars>])\n",
        "str.rstrip([<chars>])\n",
        "str.strip([<chars>])\n",
        "str.replace(<old>, <new>[, <count>])\n",
        "str.zfill(<width>)\n",
        "\n",
        "6. Fix Errors\n",
        "It probably goes without saying that you’ll need to carefully remove any errors from your data. Errors as avoidable as typos could lead to you missing out on key findings from your data. Some of these can be avoided with something as simple as a quick spell-check. \n",
        "\n",
        "Spelling mistakes or extra punctuation in data like an email address could mean you miss out on communicating with your customers. It could also lead to you sending unwanted emails to people who didn’t sign up for them. \n",
        "\n",
        "Other errors can include inconsistencies in formatting. For example, if you have a column of US dollar amounts, you’ll have to convert any other currency type into US dollars so as to preserve a consistent standard currency. The same is true of any other form of measurement such as grams, ounces, etc. \n",
        "\n",
        "7. Language Translation\n",
        "To have consistent data, you’ll want everything in the same language. \n",
        "\n",
        "The Natural Language Processing (NLP) models behind software used to analyze data are also predominantly monolingual, meaning they are not capable of processing multiple languages. So, you’ll need to translate everything into one language. \n",
        "\n",
        "8. Handle Missing Values\n",
        "When it comes to missing values you have two options:\n",
        "\n",
        "Remove the observations that have this missing value\n",
        "Input the missing data \n",
        "What you choose to do will depend on your analysis goals and what you want to do next with your data. \n",
        "\n",
        "Removing the missing value completely might remove useful insights from your data. After all, there was a reason that you wanted to pull this information in the first place.  \n",
        "\n",
        "Therefore it might be better to input the missing data by researching what should go in that field. If you don’t know what it is, you could replace it with the word missing. If it is numerical you can place a zero in the missing field. \n",
        "\n",
        "However, if there are so many missing values that there isn’t enough data to use, then you should remove the whole section. \n",
        "\n",
        "The Wrap Up\n",
        "While it can sometimes be time-consuming to clean your data, it will cost you more than just time if you skip this step. “Dirty” data can lead to a whole host of issues, so you want it clean before you begin your analysis. \n",
        "\n",
        "Once you have cleaned your data, you’ll need the right tools at hand to analyze this information. MonkeyLearn is the perfect option. \n",
        "\n",
        "Harnessing the power of AI and machine learning, MonkeyLearn tools such as the sentiment analyzer and keyword extractor allow you to analyze your qualitative data in an objective, fast way. You can also visualize all your insights in one easy-to-read place with the MonkeyLearn Studio Dashboard (pictured below)."
      ],
      "metadata": {
        "id": "RyO1aPQk2RcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YqTT_1MB7q64"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}