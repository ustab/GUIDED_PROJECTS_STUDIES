{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA HANDLING.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMj3s5m+/7q6jpWQV6lccR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ustab/GUIDED_PROJECTS_STUDIES/blob/main/DATA_HANDLING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tUNJdLqmCOT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This guide will explain the basics of what data cleaning is, then jump into the real stuff. Downstream, this guide will transform into a how-to for data cleaning with Python walking you through step by step. \n",
        "\n",
        "1. What is Data Cleaning?\n",
        "Data cleaning is the process of correcting or removing corrupt, incorrect, or unnecessary data from a data set before data analysis.\n",
        "\n",
        "Expanding on this basic definition, data cleaning, often grouped with data cleansing, data scrubbing, and data preparation, serves to turn your messy, potentially problematic data into clean data. Importantly, that’s ‘clean data’ defined as data that the powerful data analysis engines you spent money on can actually use.\n",
        "\n",
        "At MonkeyLearn, we like to explain why data cleaning is important with a simple approach: \n",
        "\n",
        "We explain the 1-100 Bad Data Principle, and \n",
        "We contextualize the principle with a couple choice quotes.\n",
        "The 1-100 Bad Data Principle \n",
        "The cost of bad data quality over time is significantly higher the longer you wait to clean.\n",
        "Every dollar spent cleaning bad data statistically prevents around $100 in down-the-line costs.\n",
        "\n",
        "For any business, ROI (return on investment) is crucial. If you invested in an advertising campaign, you’d hope to see an increase in sales.\n",
        "\n",
        "What we hope to demonstrate with the 1-100 Bad Data Principle is not only that data cleaning provides staggering ROI. We also want to drive home that it’s value is so large that investing in data analysis, and the necessary data cleaning beforehand, is often a much sounder use of resources than the sales and advertising marketing techniques of the past.\n",
        "\n",
        "As our CEO, Raúl Garreta, puts it, \n",
        "\n",
        "“If your downstream process receives garbage as input data, the quality of your results will also be bad”.\n",
        "\n",
        "It’s key to note that Raúl’s insight applies no matter the strength of your data analysis program. That would be like putting diesel into a lamborghini.\n",
        "\n",
        "With bad data it’s simple – garbage in, garbage out. \n",
        "\n",
        "Garbage in equals garbage out of powerful machine learning models.\n",
        "Now, the point of this guide is to inform you how to best perform data cleaning using Python. For the uninitiated, Python is one of the most common, if not the most common code language in the world. \n",
        "\n",
        "Furthermore, and even more significantly, the vast majority of datasets can and are programmed using Python. Compounding Python's importance, Numpy and Pandas, both Python libraries (meaning pre-programmed toolsets) are the tools of choice amongst data scientists when it comes to data cleaning, prep, and other analysis.\n",
        "\n",
        "What more does one need? Regardless, let’s get into the nitty gritty of cleaning our data with these libraries.\n",
        "\n",
        "2. Data Cleaning With Python\n",
        "Using Pandas and NumPy, we are now going to walk you through the following series of tasks, listed below. We’ll give a super-brief idea of the task, then explain the necessary code using INPUT (what you should enter) and OUTPUT (what you should see as a result). Where relevant, we’ll also have some helpful notes and tips for you to clarify tricky bits. \n",
        "\n",
        "Here are the basic data cleaning tasks we’ll tackle:\n",
        "\n",
        "Importing Libraries\n",
        "Input Customer Feedback Dataset\n",
        "Locate Missing Data\n",
        "Check for Duplicates\n",
        "Detect Outliers \n",
        "Normalize Casing \n",
        "1. Importing Libraries\n",
        "Let’s get Pandas and NumPy up and running on your Python script.\n",
        "\n",
        "INPUT:\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "OUTPUT:\n",
        "\n",
        "In this case, your script should now have the libraries loaded. You’ll see if this is true by inputting a dataset in our next step.\n",
        "\n",
        "2. Input Customer Feedback Dataset\n",
        "Next, we ask our libraries to read a feedback dataset. Let’s see what that looks like.\n",
        "\n",
        "INPUT:\n",
        "\n",
        "data = pd.read_csv('feedback.csv')\n",
        "OUTPUT: \n",
        "\n",
        "\n",
        "As you can see the “feedback.csv” should be the dataset you want to examine. And, in this case, when we read “pd.read_csv” as the prior function, we know we are using the Pandas library to read our dataset. \n",
        "\n",
        "3. Locate Missing Data\n",
        "Next, we are going to use a secret Python hack known as ‘isnull function’ to discover our data. Actually a common function, 'isnull' helps us find where in our dataset there are missing values. This is useful information as this is what we need to correct while data cleaning.\n",
        "\n",
        "INPUT:\n",
        "\n",
        "data.isnull()\n",
        "OUTPUT: \n",
        "\n",
        "\n",
        "Our output result is a list of boolean values. \n",
        "\n",
        "There are several insights the list can give us. First and foremost is where the missing data is – any ‘True’ reading under a column indicates missing data in that column’s category for that data file.\n",
        "\n",
        "So, for example, datapoint 1 has missing data in its Review section and its Review ID section (both are marked true). \n",
        "\n",
        "We can further expand the missing data of each feature by coding:\n",
        "\n",
        "INPUT: \n",
        "\n",
        "data.isnull().sum()\n",
        "OUTPUT:\n",
        "\n",
        "\n",
        "From here, we use code to actually clean the data. This boils down to two basic options. 1) Drop the data or, 2) Input missing data. If you opt to:\n",
        "\n",
        "1. Drop the data\n",
        "You’ll have to make another decision – whether to drop only the missing values and keep the data in the set, or to eliminate the feature (the entire column) wholesale because there are so many missing datapoints that it isn’t fit for analysis.\n",
        "\n",
        "If you want to drop the missing values you’ll have to go in and mark them void according to Pandas or NumBy standards (see section below). But if you want to drop the entire column, here’s the code:\n",
        "\n",
        "INPUT: \n",
        "\n",
        "remove = ['Review ID','Date']\n",
        "data.drop(remove, inplace =True, axis =1)\n",
        "OUTPUT: \n",
        "\n",
        "\n",
        "Now, let’s examine our other option.\n",
        "\n",
        "2. Input missing data\n",
        "Technically, the method described above of filling in individual values with Pandas or NumBy standards is also a form of inputting missing data – we call it adding ‘No Review’. When it comes to inputting missing data you can either add ‘No Review’ using the code below, or manually fill in the correct data.\n",
        "\n",
        "INPUT:\n",
        "\n",
        "data['Review'] = data['Review'].fillna('No review')\n",
        "OUTPUT:\n",
        "\n",
        "\n",
        "As you can see, now the data point 1 have now been marked as ‘No Review’ – success!\n",
        "\n",
        "## Check for Duplicates\n",
        "Duplicates, like missing data, cause problems and clog up analytics software. Let’s locate and eliminate them.\n",
        "\n",
        "To locate duplicates we start out with:\n",
        "\n",
        "INPUT:\n",
        "\n",
        "data.duplicated()\n",
        "\n",
        "data.drop_duplicates()\n",
        "And there we have it, our dataset with our duplicate removed. Onwards.\n",
        "\n",
        "5. Detect Outliers\n",
        "Outliers are numerical values that lie significantly outside of the statistical norm. Cutting that down from unnecessary science garble – they are data points that are so out of range they are likely misreads. \n",
        "\n",
        "They, like duplicates, need to be removed. Let’s sniff out an outlier by first, pulling up our dataset.\n",
        "\n",
        "data['Rating'].describe()\n",
        "\n",
        "Take a look at that ‘max’ value - none of the other values are even close to 100, with the mean (the average) being 11. Now, your solution to outliers will depend on your knowledge of your dataset. In this case, the data scientists who input the knowledge know that they meant to put a value of 1 not 100. So, \n",
        "\n",
        "we can safely remove the outlier to fix our data.\n",
        "#data.loc[10,'Rating'] = 1: \n",
        "Now our dataset has ratings ranging from 1 to 5, which will save major skew from if there was a rogue 100 in there.\n",
        "\n",
        "6. Normalize Casing\n",
        "\n",
        " Meaning we are going to standardize (lowercase) all review titles so as not to confuse our algorithms, and we are going to capitalize Customer Names, so that our algorithms know they are variables (you’ll see this in action below).\n",
        "\n",
        "Here’s how to make every review title lowercase:\n",
        "\n",
        "    data['Review Title'] = data['Review Title'].str.lower()\n",
        "\n",
        "Looks great! On to making sure our high-powered programs don’t get tripped up and miscategorize a customer name because it isn’t capitalized. \n",
        "\n",
        "Here’s how to ensure Customer Name capitalization:\n",
        "\n",
        "data['Customer Name'] = data['Customer Name'].str.title()\n",
        "\n",
        " We are now ready to make the most of it with our machine learning data analysis software. \n",
        "\n",
        "##Takeaways\n",
        "But your data analysis is only as good as your data cleaning, which we covered here, and its compatibility with your analysis software. With clean data and a powerful (and easy-to-use) analysis software."
      ],
      "metadata": {
        "id": "4L2AXqFemC4Y"
      }
    }
  ]
}